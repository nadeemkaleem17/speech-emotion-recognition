{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 256618,
          "sourceType": "datasetVersion",
          "datasetId": 107620
        },
        {
          "sourceId": 639622,
          "sourceType": "datasetVersion",
          "datasetId": 316368
        },
        {
          "sourceId": 653195,
          "sourceType": "datasetVersion",
          "datasetId": 325566
        },
        {
          "sourceId": 671851,
          "sourceType": "datasetVersion",
          "datasetId": 338555
        }
      ],
      "dockerImageVersionId": 29926,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  <center> Speech Emotion Recognition <center>"
      ],
      "metadata": {
        "id": "nl5C2ZEjv6QZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I am going to build a speech emotion detection classifier.\n",
        "### But first we need to learn about what is speech recognition (SER) and why are we building this project? Well, few of the reasons are-\n",
        "\n",
        "#### First, lets define SER i.e. Speech Emotion Recognition.\n",
        "* Speech Emotion Recognition, abbreviated as SER, is the act of attempting to recognize human emotion and affective states from speech. This is capitalizing on the fact that voice often reflects underlying emotion through tone and pitch. This is also the phenomenon that animals like dogs and horses employ to be able to understand human emotion.\n",
        "\n",
        "#### Why we need it?\n",
        "\n",
        "1. Emotion recognition is the part of speech recognition which is gaining more popularity and need for it increases enormously. Although there are methods to recognize emotion using machine learning techniques, this project attempts to use deep learning to recognize the emotions from data.\n",
        "\n",
        "2. SER(Speech Emotion Recognition) is used in call center for classifying calls according to emotions and can be used as the performance parameter for conversational analysis thus identifying the unsatisfied customer, customer satisfaction and so on.. for helping companies improving their services\n",
        "\n",
        "3. It can also be used in-car board system based on information of the mental state of the driver can be provided to the system to initiate his/her safety preventing accidents to happen\n",
        "\n",
        "#### Datasets used in this project\n",
        "\n",
        "* Crowd-sourced Emotional Mutimodal Actors Dataset (Crema-D)\n",
        "* Ryerson Audio-Visual Database of Emotional Speech and Song (Ravdess)\n",
        "* Surrey Audio-Visual Expressed Emotion (Savee)\n",
        "* Toronto emotional speech set (Tess)"
      ],
      "metadata": {
        "id": "whLK5-L3v6Qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "WPOp2QyAv6Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install --upgrade keras\n"
      ],
      "metadata": {
        "id": "PNJ_DbHTykkX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\n",
        "import librosa\n",
        "import librosa.display\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# to play the audio files\n",
        "from IPython.display import Audio\n",
        "\n",
        "import keras\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "y4AbTCVHv6Qd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install kaggle\n"
      ],
      "metadata": {
        "id": "QDGL0-stzSzF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# files.upload()  # Upload kaggle file that have your uername and key\n"
      ],
      "metadata": {
        "id": "h3CeriV-zWrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "iZuwnIaPzcJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "rhAshZwf5_Xn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af2fdbc9-815f-4c3e-9e11-532d60d7264a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d ejlok1/surrey-audiovisual-expressed-emotion-savee"
      ],
      "metadata": {
        "id": "n21yr4icy_wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d uwrfkaggler/ravdess-emotional-speech-audio\n",
        "!kaggle datasets download -d ejlok1/toronto-emotional-speech-set-tess\n",
        "!kaggle datasets download -d ejlok1/cremad\n"
      ],
      "metadata": {
        "id": "35HIyW-b8fE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Define dataset names\n",
        "datasets = [\n",
        "    'surrey-audiovisual-expressed-emotion-savee.zip',\n",
        "    'ravdess-emotional-speech-audio.zip',\n",
        "    'toronto-emotional-speech-set-tess.zip',\n",
        "    'cremad.zip'\n",
        "]\n",
        "\n",
        "# Unzip each dataset\n",
        "for dataset in datasets:\n",
        "    with zipfile.ZipFile(dataset, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content/' + dataset.replace('.zip', ''))\n"
      ],
      "metadata": {
        "id": "a_s1UAO98sBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "* As we are working with four different datasets, so I will be creating a dataframe storing all emotions of the data in dataframe with their paths.\n",
        "* We will use this dataframe to extract features for our model training."
      ],
      "metadata": {
        "id": "zMjGhzgRv6Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GgygQZ7o0ihQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <center> 1. Ravdess Dataframe <center>\n",
        "Here is the filename identifiers as per the official RAVDESS website:\n",
        "\n",
        "* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "* Vocal channel (01 = speech, 02 = song).\n",
        "* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
        "* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
        "* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "\n",
        "So, here's an example of an audio filename. 02-01-06-01-02-01-12.mp4\n",
        "This means the meta data for the audio file is:\n",
        "\n",
        "* Video-only (02)\n",
        "* Speech (01)\n",
        "* Fearful (06)\n",
        "* Normal intensity (01)\n",
        "* Statement \"dogs\" (02)\n",
        "* 1st Repetition (01)\n",
        "* 12th Actor (12) - Female (as the actor ID number is even)"
      ],
      "metadata": {
        "id": "gH4joUhLv6Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Savee = \"/content/surrey-audiovisual-expressed-emotion-savee/ALL/\"\n",
        "Ravdess = \"/content/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n",
        "Tess = \"/content/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
        "Crema = \"/content/cremad/AudioWAV/\"\n"
      ],
      "metadata": {
        "id": "sf0KuQSD90De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ravdess_directory_list = os.listdir(Ravdess)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "for dir in ravdess_directory_list:\n",
        "    # as their are 20 different actors in our previous directory we need to extract files for each actor.\n",
        "    actor = os.listdir(Ravdess + dir)\n",
        "    for file in actor:\n",
        "        part = file.split('.')[0]\n",
        "        part = part.split('-')\n",
        "        # third part in each file represents the emotion associated to that file.\n",
        "        file_emotion.append(int(part[2]))\n",
        "        file_path.append(Ravdess + dir + '/' + file)\n",
        "\n",
        "# dataframe for emotion of files\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "# dataframe for path of files.\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "\n",
        "# changing integers to actual emotions.\n",
        "Ravdess_df.Emotions.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\n",
        "Ravdess_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "kjXAJyjYv6Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center>2. Crema DataFrame</center>"
      ],
      "metadata": {
        "id": "rqXU4t_tv6Qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crema_directory_list = os.listdir(Crema)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for file in crema_directory_list:\n",
        "    # storing file paths\n",
        "    file_path.append(Crema + file)\n",
        "    # storing file emotions\n",
        "    part=file.split('_')\n",
        "    if part[2] == 'SAD':\n",
        "        file_emotion.append('sad')\n",
        "    elif part[2] == 'ANG':\n",
        "        file_emotion.append('angry')\n",
        "    elif part[2] == 'DIS':\n",
        "        file_emotion.append('disgust')\n",
        "    elif part[2] == 'FEA':\n",
        "        file_emotion.append('fear')\n",
        "    elif part[2] == 'HAP':\n",
        "        file_emotion.append('happy')\n",
        "    elif part[2] == 'NEU':\n",
        "        file_emotion.append('neutral')\n",
        "    else:\n",
        "        file_emotion.append('Unknown')\n",
        "\n",
        "# dataframe for emotion of files\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "# dataframe for path of files.\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "Crema_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "7gATwvcfv6Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <center> 3. TESS dataset <center>"
      ],
      "metadata": {
        "id": "iA-Uc-ARv6Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tess_directory_list = os.listdir(Tess)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for dir in tess_directory_list:\n",
        "    directories = os.listdir(Tess + dir)\n",
        "    for file in directories:\n",
        "        part = file.split('.')[0]\n",
        "        part = part.split('_')[2]\n",
        "        if part=='ps':\n",
        "            file_emotion.append('surprise')\n",
        "        else:\n",
        "            file_emotion.append(part)\n",
        "        file_path.append(Tess + dir + '/' + file)\n",
        "\n",
        "# dataframe for emotion of files\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "# dataframe for path of files.\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "Tess_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Vo85EZsfv6Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  <center> 4. SAVEE dataset <center>\n",
        "The audio files in this dataset are named in such a way that the prefix letters describes the emotion classes as follows:\n",
        "\n",
        "* 'a' = 'anger'\n",
        "* 'd' = 'disgust'\n",
        "* 'f' = 'fear'\n",
        "* 'h' = 'happiness'\n",
        "* 'n' = 'neutral'\n",
        "* 'sa' = 'sadness'\n",
        "* 'su' = 'surprise'"
      ],
      "metadata": {
        "id": "MAn9p3c2v6Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "savee_directory_list = os.listdir(Savee)\n",
        "\n",
        "file_emotion = []\n",
        "file_path = []\n",
        "\n",
        "for file in savee_directory_list:\n",
        "    file_path.append(Savee + file)\n",
        "    part = file.split('_')[1]\n",
        "    ele = part[:-6]\n",
        "    if ele=='a':\n",
        "        file_emotion.append('angry')\n",
        "    elif ele=='d':\n",
        "        file_emotion.append('disgust')\n",
        "    elif ele=='f':\n",
        "        file_emotion.append('fear')\n",
        "    elif ele=='h':\n",
        "        file_emotion.append('happy')\n",
        "    elif ele=='n':\n",
        "        file_emotion.append('neutral')\n",
        "    elif ele=='sa':\n",
        "        file_emotion.append('sad')\n",
        "    else:\n",
        "        file_emotion.append('surprise')\n",
        "\n",
        "# dataframe for emotion of files\n",
        "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
        "\n",
        "# dataframe for path of files.\n",
        "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
        "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
        "Savee_df.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "cFkX1xRFv6Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating Dataframe using all the 4 dataframes we created so far.\n",
        "data_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis = 0)\n",
        "data_path.to_csv(\"data_path.csv\",index=False)\n",
        "data_path.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "eUELB-ucv6Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualisation and Exploration"
      ],
      "metadata": {
        "id": "RxRWEx72v6Qi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's plot the count of each emotions in our dataset."
      ],
      "metadata": {
        "id": "64PITS97v6Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Count of Emotions', size=16)\n",
        "sns.countplot(x = data_path.Emotions, data = data_path)\n",
        "plt.ylabel('Count', size=12)\n",
        "plt.xlabel('Emotions', size=12)\n",
        "sns.despine(top=True, right=True, left=False, bottom=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "uyGMRzAiv6Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot waveplots and spectograms for audio signals\n",
        "\n",
        "* Waveplots - Waveplots let us know the loudness of the audio at a given time.\n",
        "* Spectograms - A spectrogram is a visual representation of the spectrum of frequencies of sound or other signals as they vary with time. It’s a representation of frequencies changing with respect to time for given audio/music signals."
      ],
      "metadata": {
        "id": "uwQnA48zv6Qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_waveplot(data, sr, e):\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)\n",
        "    librosa.display.waveshow(data, sr=sr)\n",
        "    plt.show()\n",
        "\n",
        "def create_spectrogram(data, sr, e):\n",
        "    # stft function converts the data into short term fourier transform\n",
        "    X = librosa.stft(data)\n",
        "    Xdb = librosa.amplitude_to_db(abs(X))\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n",
        "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
        "    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n",
        "    plt.colorbar()"
      ],
      "metadata": {
        "trusted": true,
        "id": "JE6_mtvKv6Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion='fear'\n",
        "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
        "data, sampling_rate = librosa.load(path)\n",
        "create_waveplot(data, sampling_rate, emotion)\n",
        "create_spectrogram(data, sampling_rate, emotion)\n",
        "Audio(path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "_nNWytqUv6Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion='angry'\n",
        "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
        "data, sampling_rate = librosa.load(path)\n",
        "create_waveplot(data, sampling_rate, emotion)\n",
        "create_spectrogram(data, sampling_rate, emotion)\n",
        "Audio(path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "vT-ZmgxIv6Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion='sad'\n",
        "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
        "data, sampling_rate = librosa.load(path)\n",
        "create_waveplot(data, sampling_rate, emotion)\n",
        "create_spectrogram(data, sampling_rate, emotion)\n",
        "Audio(path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "A4iXoXg_v6Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion='happy'\n",
        "path = np.array(data_path.Path[data_path.Emotions==emotion])[1]\n",
        "data, sampling_rate = librosa.load(path)\n",
        "create_waveplot(data, sampling_rate, emotion)\n",
        "create_spectrogram(data, sampling_rate, emotion)\n",
        "Audio(path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "1JaEXqJwv6Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "\n",
        "- Data augmentation is the process by which we create new synthetic data samples by adding small perturbations on our initial training set.\n",
        "- To generate syntactic data for audio, we can apply noise injection, shifting time, changing pitch and speed.\n",
        "- The objective is to make our model invariant to those perturbations and enhace its ability to generalize.\n",
        "- In order to this to work adding the perturbations must conserve the same label as the original training sample.\n",
        "- In images data augmention can be performed by shifting the image, zooming, rotating ...\n",
        "\n",
        "First, let's check which augmentation techniques works better for our dataset."
      ],
      "metadata": {
        "id": "tj3YtynEv6Ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def noise(data):\n",
        "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
        "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "    return data\n",
        "\n",
        "def stretch(data, rate=0.8):\n",
        "    return librosa.effects.time_stretch(data, rate=rate)\n",
        "\n",
        "def shift(data):\n",
        "    shift_range = int(np.random.uniform(low=-5, high = 5)*2000)\n",
        "    return np.roll(data, shift_range)\n",
        "\n",
        "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
        "    return librosa.effects.pitch_shift(data, sr = sampling_rate, n_steps = pitch_factor)\n",
        "\n",
        "# shift time from left or right\n",
        "def time_shift(data, sample_rate):\n",
        "    shift_max = 0.2  # shift by 20% of the total duration\n",
        "    shift = np.random.randint(int(sample_rate * shift_max))\n",
        "    direction = np.random.choice(['left', 'right'])\n",
        "    if direction == 'left':\n",
        "        shift = -shift\n",
        "    augmented_data = np.roll(data, shift)\n",
        "    return augmented_data\n",
        "\n",
        "# to reduce volume of loud sound\n",
        "def dynamic_range_compression(data):\n",
        "    compressor = np.random.uniform(0.5, 1.0)\n",
        "    data = np.sign(data) * (1 - np.exp(-compressor * np.abs(data)))\n",
        "    return data\n",
        "\n",
        "# adjust difference of frequency components\n",
        "def equalize(data, sr):\n",
        "    eq = np.random.uniform(0.8, 1.2)\n",
        "    return librosa.effects.preemphasis(data, coef=eq)\n",
        "\n",
        "# simulate effect of sound\n",
        "def reverb(data):\n",
        "    reverb_effect = np.convolve(data, np.random.rand(1000), mode='same')\n",
        "    return reverb_effect\n",
        "\n",
        "\n",
        "# taking any example and checking for techniques.\n",
        "path = np.array(data_path.Path)[1]\n",
        "data, sample_rate = librosa.load(path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "KI_xbPTwv6Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Simple Audio"
      ],
      "metadata": {
        "id": "e-y3Ex27v6Ql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=data, sr=sample_rate)\n",
        "Audio(path)"
      ],
      "metadata": {
        "trusted": true,
        "id": "lXc6otq8v6Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Noise Injection"
      ],
      "metadata": {
        "id": "pP4nAiCzv6Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = noise(data)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "trusted": true,
        "id": "i7KriGsmv6Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see noise injection is a very good augmentation technique because of which we can assure our training model is not overfitted"
      ],
      "metadata": {
        "id": "BnFTmV-Uv6Qm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Stretching"
      ],
      "metadata": {
        "id": "LpP8knL8v6Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = stretch(data)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "trusted": true,
        "id": "F32kzv-ev6Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Shifting"
      ],
      "metadata": {
        "id": "-j78qOLtv6Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = shift(data)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NcnNn0gnv6Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Pitch"
      ],
      "metadata": {
        "id": "h6wLRmlRv6Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = pitch(data, sample_rate)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "trusted": true,
        "id": "EtMHiQybv6Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Time Shift"
      ],
      "metadata": {
        "id": "PXKVZXt6VyqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = time_shift(data, sample_rate)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "hPSflwtk_RWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Dynamic Range Compression"
      ],
      "metadata": {
        "id": "s37A1DuGV3KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = dynamic_range_compression(data)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "5_jWQNwW_RTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Equalize amplitude, noise behind audio"
      ],
      "metadata": {
        "id": "lxYMZJ5sV68T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = equalize(data, sample_rate)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "np0Ik__r_RQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Reverb Audio"
      ],
      "metadata": {
        "id": "VQjiJsBSWHKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = reverb(data)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "KOW5Vh1y_RN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "56SEee1a_RGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From the above types of augmentation techniques i am using noise, stretching(ie. changing speed) and some pitching."
      ],
      "metadata": {
        "id": "ljzq1FN9v6Qn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction\n",
        "- Extraction of features is a very important part in analyzing and finding relations between different things. As we already know that the data provided of audio cannot be understood by the models directly so we need to convert them into an understandable format for which feature extraction is used.\n",
        "\n",
        "\n",
        "The audio signal is a three-dimensional signal in which three axes represent time, amplitude and frequency.\n",
        "\n",
        "![image.png](https://miro.medium.com/max/633/1*7sKM9aECRmuoqTadCYVw9A.jpeg)\n",
        "\n",
        "I am no expert on audio signals and feature extraction on audio files so i need to search and found a very good blog written by [Askash Mallik](https://medium.com/heuristics/audio-signal-feature-extraction-and-clustering-935319d2225) on feature extraction.\n",
        "\n",
        "As stated there with the help of the sample rate and the sample data, one can perform several transformations on it to extract valuable features out of it.\n",
        "1. Zero Crossing Rate : The rate of sign-changes of the signal during the duration of a particular frame.\n",
        "2. Energy : The sum of squares of the signal values, normalized by the respective frame length.\n",
        "3. Entropy of Energy : The entropy of sub-frames’ normalized energies. It can be interpreted as a measure of abrupt changes.\n",
        "4. Spectral Centroid : The center of gravity of the spectrum.\n",
        "5. Spectral Spread : The second central moment of the spectrum.\n",
        "6. Spectral Entropy :  Entropy of the normalized spectral energies for a set of sub-frames.\n",
        "7. Spectral Flux : The squared difference between the normalized magnitudes of the spectra of the two successive frames.\n",
        "8. Spectral Rolloff : The frequency below which 90% of the magnitude distribution of the spectrum is concentrated.\n",
        "9.  MFCCs Mel Frequency Cepstral Coefficients form a cepstral representation where the frequency bands are not linear but distributed according to the mel-scale.\n",
        "10. Chroma Vector : A 12-element representation of the spectral energy where the bins represent the 12 equal-tempered pitch classes of western-type music (semitone spacing).\n",
        "11. Chroma Deviation : The standard deviation of the 12 chroma coefficients.\n",
        "\n",
        "\n",
        "In this project i am not going deep in feature selection process to check which features are good for our dataset rather i am only extracting 5 features:\n",
        "- Zero Crossing Rate\n",
        "- Chroma_stft\n",
        "- MFCC\n",
        "- RMS(root mean square) value\n",
        "- MelSpectogram to train our model.\n",
        "- Spectral bandwidth\n",
        "- Contrast"
      ],
      "metadata": {
        "id": "Ma2hL4Dgv6Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(data):\n",
        "    # ZCR\n",
        "    result = np.array([])\n",
        "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
        "    result=np.hstack((result, zcr)) # stacking horizontally\n",
        "\n",
        "    # Chroma_stft\n",
        "    stft = np.abs(librosa.stft(data))\n",
        "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
        "\n",
        "    # MFCC\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, mfcc)) # stacking horizontally\n",
        "\n",
        "    # Root Mean Square Value\n",
        "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
        "    result = np.hstack((result, rms)) # stacking horizontally\n",
        "\n",
        "    # MelSpectogram\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, mel)) # stacking horizontally\n",
        "\n",
        "    # # Spectral Centroid\n",
        "\n",
        "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=data, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, spectral_centroid))\n",
        "\n",
        "    # Spectral Bandwidth\n",
        "\n",
        "    # spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=data, sr=sample_rate).T, axis=0)\n",
        "    # result = np.hstack((result, spectral_bandwidth))\n",
        "\n",
        "    # Spectral Contrast\n",
        "\n",
        "    # spectral_contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T, axis=0)\n",
        "    # result = np.hstack((result, spectral_contrast))\n",
        "\n",
        "    # Spectral Roll-off\n",
        "    # spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=data, sr=sample_rate, roll_percent=0.85).T, axis=0)\n",
        "    # result = np.hstack((result, spectral_rolloff))\n",
        "\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "iobq5ogIv6Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(path):\n",
        "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
        "\n",
        "    # without augmentation\n",
        "    res1 = extract_features(data)\n",
        "    result = np.array(res1)\n",
        "\n",
        "    # data with noise\n",
        "    noise_data = noise(data)\n",
        "    res2 = extract_features(noise_data)\n",
        "    result = np.vstack((result, res2))\n",
        "\n",
        "    # data with stretching and pitching\n",
        "    new_data = stretch(data)\n",
        "    data_stretch_pitch = pitch(new_data, sample_rate)\n",
        "    res3 = extract_features(data_stretch_pitch)\n",
        "    result = np.vstack((result, res3))\n",
        "\n",
        "    # data with time shifting\n",
        "    shifted_data = time_shift(data, sample_rate)\n",
        "    res4 = extract_features(shifted_data)\n",
        "    result = np.vstack((result, res4))\n",
        "\n",
        "    # data with dynamic range compression\n",
        "    compressed_data = dynamic_range_compression(data)\n",
        "    res5 = extract_features(compressed_data)\n",
        "    result = np.vstack((result, res5))\n",
        "\n",
        "    # data with equalization\n",
        "    equalized_data = equalize(data, sample_rate)\n",
        "    res6 = extract_features(equalized_data)\n",
        "    result = np.vstack((result, res6))\n",
        "\n",
        "    # # data with reverb\n",
        "    # reverb_data = reverb(data)\n",
        "    # res7 = extract_features(reverb_data)\n",
        "    # result = np.vstack((result, res7))\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "L2DDB0P5_biz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here I extracted features from audi files\n",
        "* This feature extraction was taking time.\n",
        "* To solve time problem I extracted them once and saved in .npy files\n",
        "* After Features extraction, loaded the .npy X,Y and used them"
      ],
      "metadata": {
        "id": "G0eVW0epWiA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X, Y = [], []\n",
        "# ind = 1\n",
        "# for path, emotion in zip(data_path.Path, data_path.Emotions):\n",
        "#     feature = get_features(path)\n",
        "#     print(ind)\n",
        "#     ind += 1\n",
        "#     for ele in feature:\n",
        "#         X.append(ele)\n",
        "#         # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n",
        "#         Y.append(emotion)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9qgnFrwuv6Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################# FOR SAVING IN numpy ##################################\n",
        "\n",
        "\n",
        "\n",
        "# # Convert lists to numpy arrays\n",
        "# X_array = np.array(X)\n",
        "# Y_array = np.array(Y)\n",
        "# np.save('/content/drive/MyDrive/My Folder/X_features.npy', X_array)\n",
        "# np.save('/content/drive/MyDrive/My Folder/Y_labels.npy', Y_array)\n",
        "# # Save arrays to .npy files\n",
        "# np.save('/content/X_features.npy', X_array)\n",
        "# np.save('/content/Y_labels.npy', Y_array)\n",
        "\n"
      ],
      "metadata": {
        "id": "KNKsu4AhaBvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ############################# FOR LOADING Numpy X AND Y ##################################\n",
        "# X_array = np.load('/content/X_features.npy')\n",
        "# Y_array = np.load('/content/Y_labels.npy')\n",
        "\n",
        "# # If you want them as lists again\n",
        "# X = X_array.tolist()\n",
        "# Y = Y_array.tolist()\n"
      ],
      "metadata": {
        "id": "4Dnw5yhLaUV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Here I  mounted google drive\n",
        "* Uploaded files from google drive\n",
        "* Features Extracted files will be in the project folder"
      ],
      "metadata": {
        "id": "ClFVYwjmXHYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gMHtjFx2G2p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################# FOR LOADING Numpy X AND Y ##################################\n",
        "\n",
        "\n",
        "# X_array1 = np.load('/content/drive/MyDrive/My Folder/X_features.npy')\n",
        "# Y_array1 = np.load('/content/drive/MyDrive/My Folder/Y_labels.npy')\n",
        "\n",
        "# # If you want them as lists again\n",
        "# X = X_array1.tolist()\n",
        "# Y = Y_array1.tolist()\n"
      ],
      "metadata": {
        "id": "kGlEdlTUcPlV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jftq9XQ0ceOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Features = pd.DataFrame(X)\n",
        "Features['labels'] = Y\n",
        "Features.to_csv('features.csv', index=False)\n",
        "Features.sample(5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "I7gYdlQPv6Qt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "7243a629-e77a-4b0a-bacd-aef71716acac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              0         1         2         3         4         5         6  \\\n",
              "67897  0.141594  0.546109  0.468488  0.488112  0.553716  0.645137  0.618634   \n",
              "69513  0.025866  0.332503  0.334339  0.232307  0.243735  0.249915  0.419978   \n",
              "31830  0.055913  0.633165  0.558486  0.488095  0.501623  0.568332  0.503402   \n",
              "29937  0.056570  0.560360  0.585672  0.535939  0.548877  0.666266  0.688347   \n",
              "26384  0.117983  0.551488  0.726139  0.717381  0.669638  0.476331  0.354386   \n",
              "\n",
              "              7         8         9  ...           154           155  \\\n",
              "67897  0.491209  0.481875  0.582355  ...  1.710261e-04  1.648473e-04   \n",
              "69513  0.501640  0.418838  0.513710  ...  5.010732e-05  3.217035e-05   \n",
              "31830  0.483069  0.558058  0.710390  ...  9.005706e-10  8.505473e-10   \n",
              "29937  0.669565  0.608932  0.600284  ...  9.219977e-05  9.007699e-05   \n",
              "26384  0.373672  0.463651  0.513924  ...  3.170504e-07  2.889315e-07   \n",
              "\n",
              "                156           157           158           159           160  \\\n",
              "67897  1.461626e-04  1.455819e-04  1.280692e-04  1.382304e-04  1.362704e-04   \n",
              "69513  4.106615e-05  5.032610e-05  3.796028e-05  4.278753e-05  2.899580e-05   \n",
              "31830  8.131426e-10  7.851890e-10  7.650596e-10  7.503808e-10  7.408779e-10   \n",
              "29937  8.818944e-05  8.656411e-05  8.526849e-05  8.421528e-05  8.349225e-05   \n",
              "26384  2.687297e-07  2.539808e-07  2.439559e-07  2.383800e-07  1.907924e-07   \n",
              "\n",
              "                161          162  labels  \n",
              "67897  1.207512e-04  3090.549694   happy  \n",
              "69513  2.281984e-06  1169.795486     sad  \n",
              "31830  7.349818e-10  1428.732976     sad  \n",
              "29937  8.300414e-05  1280.026057   happy  \n",
              "26384  7.317811e-08  2025.644292   angry  \n",
              "\n",
              "[5 rows x 164 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b85fd88-f223-4c51-a7ed-e82ce640f47a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>67897</th>\n",
              "      <td>0.141594</td>\n",
              "      <td>0.546109</td>\n",
              "      <td>0.468488</td>\n",
              "      <td>0.488112</td>\n",
              "      <td>0.553716</td>\n",
              "      <td>0.645137</td>\n",
              "      <td>0.618634</td>\n",
              "      <td>0.491209</td>\n",
              "      <td>0.481875</td>\n",
              "      <td>0.582355</td>\n",
              "      <td>...</td>\n",
              "      <td>1.710261e-04</td>\n",
              "      <td>1.648473e-04</td>\n",
              "      <td>1.461626e-04</td>\n",
              "      <td>1.455819e-04</td>\n",
              "      <td>1.280692e-04</td>\n",
              "      <td>1.382304e-04</td>\n",
              "      <td>1.362704e-04</td>\n",
              "      <td>1.207512e-04</td>\n",
              "      <td>3090.549694</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69513</th>\n",
              "      <td>0.025866</td>\n",
              "      <td>0.332503</td>\n",
              "      <td>0.334339</td>\n",
              "      <td>0.232307</td>\n",
              "      <td>0.243735</td>\n",
              "      <td>0.249915</td>\n",
              "      <td>0.419978</td>\n",
              "      <td>0.501640</td>\n",
              "      <td>0.418838</td>\n",
              "      <td>0.513710</td>\n",
              "      <td>...</td>\n",
              "      <td>5.010732e-05</td>\n",
              "      <td>3.217035e-05</td>\n",
              "      <td>4.106615e-05</td>\n",
              "      <td>5.032610e-05</td>\n",
              "      <td>3.796028e-05</td>\n",
              "      <td>4.278753e-05</td>\n",
              "      <td>2.899580e-05</td>\n",
              "      <td>2.281984e-06</td>\n",
              "      <td>1169.795486</td>\n",
              "      <td>sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31830</th>\n",
              "      <td>0.055913</td>\n",
              "      <td>0.633165</td>\n",
              "      <td>0.558486</td>\n",
              "      <td>0.488095</td>\n",
              "      <td>0.501623</td>\n",
              "      <td>0.568332</td>\n",
              "      <td>0.503402</td>\n",
              "      <td>0.483069</td>\n",
              "      <td>0.558058</td>\n",
              "      <td>0.710390</td>\n",
              "      <td>...</td>\n",
              "      <td>9.005706e-10</td>\n",
              "      <td>8.505473e-10</td>\n",
              "      <td>8.131426e-10</td>\n",
              "      <td>7.851890e-10</td>\n",
              "      <td>7.650596e-10</td>\n",
              "      <td>7.503808e-10</td>\n",
              "      <td>7.408779e-10</td>\n",
              "      <td>7.349818e-10</td>\n",
              "      <td>1428.732976</td>\n",
              "      <td>sad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29937</th>\n",
              "      <td>0.056570</td>\n",
              "      <td>0.560360</td>\n",
              "      <td>0.585672</td>\n",
              "      <td>0.535939</td>\n",
              "      <td>0.548877</td>\n",
              "      <td>0.666266</td>\n",
              "      <td>0.688347</td>\n",
              "      <td>0.669565</td>\n",
              "      <td>0.608932</td>\n",
              "      <td>0.600284</td>\n",
              "      <td>...</td>\n",
              "      <td>9.219977e-05</td>\n",
              "      <td>9.007699e-05</td>\n",
              "      <td>8.818944e-05</td>\n",
              "      <td>8.656411e-05</td>\n",
              "      <td>8.526849e-05</td>\n",
              "      <td>8.421528e-05</td>\n",
              "      <td>8.349225e-05</td>\n",
              "      <td>8.300414e-05</td>\n",
              "      <td>1280.026057</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26384</th>\n",
              "      <td>0.117983</td>\n",
              "      <td>0.551488</td>\n",
              "      <td>0.726139</td>\n",
              "      <td>0.717381</td>\n",
              "      <td>0.669638</td>\n",
              "      <td>0.476331</td>\n",
              "      <td>0.354386</td>\n",
              "      <td>0.373672</td>\n",
              "      <td>0.463651</td>\n",
              "      <td>0.513924</td>\n",
              "      <td>...</td>\n",
              "      <td>3.170504e-07</td>\n",
              "      <td>2.889315e-07</td>\n",
              "      <td>2.687297e-07</td>\n",
              "      <td>2.539808e-07</td>\n",
              "      <td>2.439559e-07</td>\n",
              "      <td>2.383800e-07</td>\n",
              "      <td>1.907924e-07</td>\n",
              "      <td>7.317811e-08</td>\n",
              "      <td>2025.644292</td>\n",
              "      <td>angry</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 164 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b85fd88-f223-4c51-a7ed-e82ce640f47a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6b85fd88-f223-4c51-a7ed-e82ce640f47a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6b85fd88-f223-4c51-a7ed-e82ce640f47a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d2e63d7d-7661-4faf-b0da-d6de91d5d386\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d2e63d7d-7661-4faf-b0da-d6de91d5d386')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d2e63d7d-7661-4faf-b0da-d6de91d5d386 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Features['labels'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7p3HLDN4svE",
        "outputId": "362914bf-3175-467c-efa5-ed00fa55ad26"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['disgust', 'happy', 'sad', 'angry', 'fear', 'calm', 'surprise',\n",
              "       'neutral'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We have applied data augmentation and extracted the features for each audio files and saved them."
      ],
      "metadata": {
        "id": "VgKaACcvv6Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "- As of now we have extracted the data, now we need to normalize and split our data for training and testing."
      ],
      "metadata": {
        "id": "JyNGV0cIv6Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = Features.iloc[: ,:-1].values\n",
        "Y = Features['labels'].values\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "UGEkVpN5v6Qt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As this is a multiclass classification problem onehotencoding our Y.\n",
        "encoder = encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
      ],
      "metadata": {
        "trusted": true,
        "id": "DwfDHQCrv6Qt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "fncaw0ZWv6Qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f927d85-6f5e-4acd-85be-bf43a23c96ed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((54729, 163), (54729, 8), (18243, 163), (18243, 8))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling our data with sklearn's Standard scaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "EM8jZDOPv6Qt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9fd1e0-ae60-485f-8ed2-352939616496"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((54729, 163), (54729, 8), (18243, 163), (18243, 8))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making our data compatible to model.\n",
        "x_train = np.expand_dims(x_train, axis=2)\n",
        "x_test = np.expand_dims(x_test, axis=2)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "HVjzVSymv6Qu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a388bdb-08fb-486a-fd3d-f2a3c77ec114"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((54729, 163, 1), (54729, 8), (18243, 163, 1), (18243, 8))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "G5_0gi6Bv6Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dropout, Dense, GRU, LSTM, GlobalAveragePooling1D\n",
        "def build_cnn(input_shape):\n",
        "  # input_shape=input_shape (x_train.shape[1], 1)\n",
        "  model=Sequential()\n",
        "  model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=input_shape))\n",
        "  model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "  model.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))  # Increased filters\n",
        "  model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
        "\n",
        "  model.add(Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "  model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "  model.add(Dropout(0.3))\n",
        "\n",
        "  model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "  model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "K9GQA0_pv6Qu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_lstm_layers(model):\n",
        "    model.add(Dense(100, activation='relu'))  # Adding Dense layer before LSTM\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Adding GRU Layer\n",
        "    model.add(LSTM(100, return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Adding LSTM Layer\n",
        "    model.add(GRU(50, return_sequences=True))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(LSTM(50, return_sequences=False))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Adding Dense Layers\n",
        "    model.add(Dense(units=32, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Output Layer\n",
        "    model.add(Dense(units=8, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "CxCNEAED8z-a"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input shape\n",
        "input_shape = (x_train.shape[1], 1)  # Adjust as per your data\n",
        "\n",
        "# Build the CNN model\n",
        "cnn_model = build_cnn(input_shape)\n",
        "\n",
        "# Add LSTM layers to the CNN model\n",
        "model = add_lstm_layers(cnn_model)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "i5wpN3Ub9L_R",
        "outputId": "589a8946-f9ec-4c3d-c4c5-7732e430b3cb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m163\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │           \u001b[38;5;34m1,536\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m82\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m82\u001b[0m, \u001b[38;5;34m512\u001b[0m)             │         \u001b[38;5;34m655,872\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m512\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │         \u001b[38;5;34m393,472\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m256\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │         \u001b[38;5;34m163,968\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │          \u001b[38;5;34m41,024\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m100\u001b[0m)              │           \u001b[38;5;34m6,500\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m100\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m100\u001b[0m)              │          \u001b[38;5;34m80,400\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m100\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (\u001b[38;5;33mGRU\u001b[0m)                            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m50\u001b[0m)               │          \u001b[38;5;34m22,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m50\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │          \u001b[38;5;34m20,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m1,632\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │             \u001b[38;5;34m264\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">163</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">82</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">82</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">393,472</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">41,024</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,500</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">80,400</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">22,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,632</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">264</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,387,668\u001b[0m (5.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,387,668</span> (5.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,387,668\u001b[0m (5.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,387,668</span> (5.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n",
        "history=model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp])"
      ],
      "metadata": {
        "trusted": true,
        "id": "6PhhKgfnv6Qu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d4d5880-0238-4638-ebb2-f1fda439fc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m954s\u001b[0m 1s/step - accuracy: 0.2496 - loss: 1.8319 - val_accuracy: 0.4056 - val_loss: 1.4690 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m965s\u001b[0m 1s/step - accuracy: 0.4048 - loss: 1.4888 - val_accuracy: 0.4403 - val_loss: 1.3849 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m968s\u001b[0m 1s/step - accuracy: 0.4430 - loss: 1.3896 - val_accuracy: 0.4702 - val_loss: 1.2919 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m928s\u001b[0m 1s/step - accuracy: 0.4732 - loss: 1.3282 - val_accuracy: 0.5052 - val_loss: 1.2171 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m933s\u001b[0m 1s/step - accuracy: 0.4948 - loss: 1.2737 - val_accuracy: 0.5155 - val_loss: 1.2002 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m271/856\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:51\u001b[0m 1s/step - accuracy: 0.5150 - loss: 1.2282"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n",
        "\n",
        "epochs = [i for i in range(50)]\n",
        "fig , ax = plt.subplots(1,2)\n",
        "train_acc = history.history['accuracy']\n",
        "train_loss = history.history['loss']\n",
        "test_acc = history.history['val_accuracy']\n",
        "test_loss = history.history['val_loss']\n",
        "\n",
        "fig.set_size_inches(20,6)\n",
        "ax[0].plot(epochs , train_loss , label = 'Training Loss')\n",
        "ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n",
        "ax[0].set_title('Training & Testing Loss')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "\n",
        "ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n",
        "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n",
        "ax[1].set_title('Training & Testing Accuracy')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "lRSgo2APv6Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting on test data.\n",
        "pred_test = model.predict(x_test)\n",
        "y_pred = encoder.inverse_transform(pred_test)\n",
        "\n",
        "y_test = encoder.inverse_transform(y_test)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "OlHQ52Kwv6Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
        "df['Predicted Labels'] = y_pred.flatten()\n",
        "df['Actual Labels'] = y_test.flatten()\n",
        "\n",
        "df.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "4glGFEbRv6Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize = (12, 10))\n",
        "cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n",
        "sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n",
        "plt.title('Confusion Matrix', size=20)\n",
        "plt.xlabel('Predicted Labels', size=14)\n",
        "plt.ylabel('Actual Labels', size=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "TjHIr-aMv6Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "trusted": true,
        "id": "MSRhiMSXv6Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can see our model is more accurate in predicting surprise, angry emotions and it makes sense also because audio files of these emotions differ to other audio files in a lot of ways like pitch, speed etc..\n",
        "- We overall achieved 68% accuracy on our test data and its decent but we can improve it more by applying more augmentation techniques and using other feature extraction methods. You can try more possible methods"
      ],
      "metadata": {
        "id": "AbHhLBmIv6Qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WORKING ON THE NEW AUDIO FILE\n",
        "* Load Saved model\n",
        "* Process audio file using feature extractions\n",
        "* Take audio file path and do processing and prediction\n",
        "* Convert prediction into emotion"
      ],
      "metadata": {
        "id": "_iFzqHGWY6yb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # You can Load from anywhere else you saved the model\n",
        "model = load_model('/content/drive/MyDrive/My Folder/my_model1.keras')"
      ],
      "metadata": {
        "id": "okS5dTzKYUNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_new_audio_file(file_path):\n",
        "    features = get_features(file_path)\n",
        "    return features\n",
        "\n",
        "def prepare_input_for_model(features):\n",
        "    feature_vector = features[0]\n",
        "    # Reshape to match the input shape of the model\n",
        "    feature_vector = np.expand_dims(feature_vector, axis=1)\n",
        "    return feature_vector\n"
      ],
      "metadata": {
        "id": "S2EY9Te5Ye-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'angry_anime'\n",
        "file_path = f'/content/{filename}.wav'\n",
        "\n",
        "def predict_emotion(file_path, model):\n",
        "    features = process_new_audio_file(file_path)\n",
        "    input_for_model = prepare_input_for_model(features)\n",
        "\n",
        "    # Predict emotion\n",
        "    prediction = model.predict(np.array([input_for_model]))\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "X7i-KzYtYkKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "predicted_array = predict_emotion(file_path, model)\n",
        "prediction = np.array(predicted_array)\n",
        "predicted_index = np.argmax(prediction)\n",
        "# Emotions labeled using the encoder. You can see in encoder.categories_[0]\n",
        "emotion_labels = ['angry', 'calm', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
        "\n",
        "predicted_emotion = emotion_labels[predicted_index]\n",
        "print(\"Predicted emotion:\", predicted_emotion)\n"
      ],
      "metadata": {
        "id": "0NcUZ6sSYF7-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}